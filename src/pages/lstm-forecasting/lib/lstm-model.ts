import * as tf from '@tensorflow/tfjs';
import type { 
  TimeSeriesDataPoint, 
  LSTMParams, 
  LSTMModelConfig, 
  TrainedLSTMModel, 
  LSTMForecastResult, 
  ValidationMetrics, 
  PreprocessedData,
  ProcessedFeatures,
  FeatureConfig
} from '../types/lstm-forecasting';
import { LSTMDataProcessor } from './data-processor';

export class LSTMModel {
  private model: tf.Sequential | null = null;
  private params: LSTMParams;
  private config: LSTMModelConfig;
  private preprocessedData: PreprocessedData | null = null;
  private isCompiled: boolean = false;

  constructor(params: LSTMParams, config: LSTMModelConfig) {
    this.params = params;
    this.config = config;
  }

  static createModel(params: LSTMParams, config: LSTMModelConfig): LSTMModel {
    return new LSTMModel(params, config);
  }

  // Build the LSTM neural network architecture
  private buildModel(inputShape: [number, number]): tf.Sequential {
    console.log('üèóÔ∏è Building LSTM model architecture...');
    console.log(`üìê Input shape: [${inputShape[0]}, ${inputShape[1]}]`);

    const model = tf.sequential();

    // First LSTM layer
    model.add(tf.layers.lstm({
      units: this.params.hiddenUnits,
      returnSequences: this.params.layers > 1, // Return sequences if we have more layers
      inputShape: inputShape,
      dropout: this.params.dropout,
      recurrentDropout: this.params.dropout * 0.5,
      kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
      recurrentRegularizer: tf.regularizers.l2({ l2: 0.001 }),
    }));

    // Additional LSTM layers
    for (let i = 1; i < this.params.layers; i++) {
      model.add(tf.layers.lstm({
        units: Math.max(Math.floor(this.params.hiddenUnits / (i + 1)), 32), // Decreasing units
        returnSequences: i < this.params.layers - 1, // Return sequences except for the last layer
        dropout: this.params.dropout,
        recurrentDropout: this.params.dropout * 0.5,
        kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
        recurrentRegularizer: tf.regularizers.l2({ l2: 0.001 }),
      }));
    }

    // Batch normalization for stability
    model.add(tf.layers.batchNormalization());

    // Dense layers for final prediction
    model.add(tf.layers.dense({
      units: Math.max(Math.floor(this.params.hiddenUnits / 2), 16),
      activation: 'relu',
      kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
    }));

    model.add(tf.layers.dropout({ rate: this.params.dropout }));

    // Output layer
    model.add(tf.layers.dense({
      units: this.config.forecastHorizon, // Predict multiple days if needed
      activation: 'linear', // Linear for regression
    }));

    console.log('‚úÖ LSTM model architecture built');
    return model;
  }

  // Compile the model with optimizer and loss function
  private compileModel(): void {
    if (!this.model) {
      throw new Error('Model not built yet');
    }

    console.log('‚öôÔ∏è Compiling LSTM model...');

    // Use Adam optimizer with learning rate scheduling
    const optimizer = tf.train.adam(this.params.learningRate);

    this.model.compile({
      optimizer: optimizer,
      loss: 'meanSquaredError',
      metrics: ['mae', 'mse'],
    });

    this.isCompiled = true;
    console.log('‚úÖ Model compiled successfully');
  }

  // Train the LSTM model
  async train(
    timeSeries: TimeSeriesDataPoint[],
    onProgress?: (epoch: number, logs: any) => void
  ): Promise<TrainedLSTMModel> {
    const startTime = Date.now();
    
    console.log('üöÄ Starting LSTM model training...');
    console.log(`üìä Dataset: ${timeSeries.length} data points`);
    console.log(`üîß Parameters: ${JSON.stringify(this.params, null, 2)}`);

    // Step 1: Feature engineering
    const featureConfig: FeatureConfig = {
      includeLags: true,
      lagDays: [1, 2, 3, 7, 14], // 1-3 days, 1-2 weeks
      includeMovingAverages: true,
      movingAverageWindows: [3, 7, 14, 30], // 3 days to 1 month
      includeSeasonality: this.config.includeSeasonality,
      seasonalPeriods: [7, 30], // Weekly and monthly seasonality
      includeTrend: true,
      includeWeekday: true,
      includeMonth: true,
      includeHolidays: this.config.includeExternalFeatures,
    };

    const processedFeatures = LSTMDataProcessor.engineerFeatures(timeSeries, featureConfig);

    // Step 2: Create sequences
    this.preprocessedData = LSTMDataProcessor.createSequences(
      processedFeatures,
      this.params.lookBackDays,
      this.config.forecastHorizon
    );

    // Step 3: Normalize data
    if (this.config.scaleData) {
      this.preprocessedData = LSTMDataProcessor.normalizeData(this.preprocessedData);
    }

    const { sequences, targets, metadata } = this.preprocessedData;

    // Step 4: Convert to tensors
    console.log('üîÑ Converting data to tensors...');
    const inputTensor = tf.tensor3d(sequences); // [samples, timesteps, features]
    const outputTensor = tf.tensor2d(targets); // [samples, forecast_horizon]

    console.log(`üìê Input tensor shape: ${inputTensor.shape}`);
    console.log(`üìê Output tensor shape: ${outputTensor.shape}`);

    // Step 5: Split data
    const trainSize = metadata.trainSize;
    const trainX = inputTensor.slice([0, 0, 0], [trainSize, -1, -1]);
    const trainY = outputTensor.slice([0, 0], [trainSize, -1]);
    const testX = inputTensor.slice([trainSize, 0, 0], [-1, -1, -1]);
    const testY = outputTensor.slice([trainSize, 0], [-1, -1]);

    console.log(`üìä Training samples: ${trainSize}, Test samples: ${sequences.length - trainSize}`);

    // Step 6: Build and compile model
    this.model = this.buildModel([this.params.lookBackDays, metadata.featuresCount]);
    this.compileModel();

    // Step 7: Setup callbacks
    const trainingHistory: { loss: number[]; valLoss: number[]; epochs: number[] } = {
      loss: [],
      valLoss: [],
      epochs: [],
    };

    let bestValLoss = Infinity;
    let bestEpoch = 0;
    let patience = 0;
    const maxPatience = 20; // Early stopping patience

    const customCallback = {
      onEpochEnd: async (epoch: number, logs: any) => {
        const loss = Number(logs?.loss) || 0;
        const valLoss = Number(logs?.val_loss) || 0;

        trainingHistory.loss.push(loss);
        trainingHistory.valLoss.push(valLoss);
        trainingHistory.epochs.push(epoch);

        // Early stopping logic
        if (valLoss < bestValLoss) {
          bestValLoss = valLoss;
          bestEpoch = epoch;
          patience = 0;
        } else {
          patience++;
        }

        // Call progress callback
        if (onProgress) {
          onProgress(epoch, {
            ...logs,
            bestValLoss,
            bestEpoch,
            patience,
            maxPatience,
          });
        }

        console.log(`Epoch ${epoch + 1}/${this.params.epochs} - loss: ${loss.toFixed(6)} - val_loss: ${valLoss.toFixed(6)} - patience: ${patience}/${maxPatience}`);

        // Early stopping
        if (patience >= maxPatience) {
          console.log(`üõë Early stopping at epoch ${epoch + 1}`);
          this.model!.stopTraining = true;
        }
      },
    };

    // Step 8: Train the model
    console.log('üéØ Training neural network...');
    await this.model.fit(trainX, trainY, {
      epochs: this.params.epochs,
      batchSize: this.params.batchSize,
      validationData: [testX, testY],
      validationSplit: this.params.validationSplit,
      shuffle: true,
      verbose: 0,
      callbacks: [customCallback],
    });

    // Step 9: Calculate validation metrics
    console.log('üìä Calculating validation metrics...');
    const predictions = this.model.predict(testX) as tf.Tensor;
    const testPredictions = await predictions.data();
    const testActuals = await testY.data();

    // Convert back to original scale if normalized
    let denormalizedPredictions = Array.from(testPredictions);
    let denormalizedActuals = Array.from(testActuals);

    if (this.config.scaleData && this.preprocessedData.targetScaler) {
      denormalizedPredictions = LSTMDataProcessor.denormalizePredictions(
        denormalizedPredictions,
        this.preprocessedData.targetScaler
      );
      denormalizedActuals = LSTMDataProcessor.denormalizePredictions(
        denormalizedActuals,
        this.preprocessedData.targetScaler
      );
    }

    const validationMetrics = this.calculateValidationMetrics(
      denormalizedActuals,
      denormalizedPredictions
    );

    // Step 10: Create trained model object
    const modelSummary = this.getModelSummary();
    const trainingTime = Date.now() - startTime;

    const trainedModel: TrainedLSTMModel = {
      id: `lstm_${Date.now()}`,
      name: `LSTM(${this.params.hiddenUnits}x${this.params.layers}) - ${this.params.lookBackDays}d lookback`,
      type: 'LSTM',
      params: this.params,
      modelArchitecture: {
        inputShape: [this.params.lookBackDays, metadata.featuresCount],
        outputShape: [this.config.forecastHorizon],
        totalParams: modelSummary.totalParams,
        trainableParams: modelSummary.trainableParams,
      },
      trainingHistory: {
        ...trainingHistory,
        bestEpoch,
        bestValLoss,
      },
      trainedAt: new Date(),
      trainingTime,
      convergenceStatus: patience >= maxPatience ? 'early_stopped' : 'completed',
      validationMetrics,
      scalers: this.config.scaleData ? {
        feature: this.preprocessedData.scaler,
        target: this.preprocessedData.targetScaler,
      } : undefined,
    };

    // Cleanup tensors
    inputTensor.dispose();
    outputTensor.dispose();
    trainX.dispose();
    trainY.dispose();
    testX.dispose();
    testY.dispose();
    predictions.dispose();

    console.log('‚úÖ LSTM training completed successfully!');
    console.log(`üìà Final metrics: R¬≤=${validationMetrics.r2.toFixed(3)}, MAPE=${validationMetrics.mape.toFixed(1)}%, RMSE=${validationMetrics.rmse.toFixed(2)}`);
    console.log(`‚è±Ô∏è Training time: ${(trainingTime / 1000).toFixed(1)}s`);

    return trainedModel;
  }

  // Generate forecasts
  async forecast(
    timeSeries: TimeSeriesDataPoint[],
    horizon: number = this.config.forecastHorizon
  ): Promise<LSTMForecastResult> {
    if (!this.model || !this.preprocessedData) {
      throw new Error('Model must be trained before forecasting');
    }

    console.log(`üîÆ Generating LSTM forecast for ${horizon} periods...`);

    // Use the last sequence from the training data as the starting point
    const { sequences, featureNames, targetScaler } = this.preprocessedData;
    const lastSequence = sequences[sequences.length - 1];

    // Generate forecasts iteratively
    const predictions: number[] = [];
    let currentSequence = [...lastSequence];

    for (let step = 0; step < horizon; step++) {
      // Prepare input tensor
      const inputTensor = tf.tensor3d([currentSequence]);
      
      // Make prediction
      const prediction = this.model.predict(inputTensor) as tf.Tensor;
      const predictionValue = await prediction.data();
      
      // Get the next day prediction (first output if multi-output)
      const nextValue = predictionValue[0];
      predictions.push(nextValue);

      // Update sequence for next prediction (sliding window)
      // Remove first timestep and add new prediction as last timestep
      currentSequence = currentSequence.slice(1);
      
      // Create next timestep features (simplified - in production should engineer proper features)
      const nextTimestep = new Array(featureNames.length).fill(0);
      nextTimestep[0] = nextValue; // Set predicted value as main feature
      currentSequence.push(nextTimestep);

      // Cleanup
      inputTensor.dispose();
      prediction.dispose();
    }

    // Denormalize predictions if necessary
    let denormalizedPredictions = predictions;
    if (this.config.scaleData && targetScaler) {
      denormalizedPredictions = LSTMDataProcessor.denormalizePredictions(
        predictions,
        targetScaler
      );
    }

    // Create prediction time series points
    const lastDate = timeSeries[timeSeries.length - 1].date;
    const predictionPoints: TimeSeriesDataPoint[] = denormalizedPredictions.map((value, index) => {
      const forecastDate = new Date(lastDate.getTime() + (index + 1) * 24 * 60 * 60 * 1000);
      return {
        date: forecastDate,
        timestamp: forecastDate.getTime(),
        value: Math.max(0, value), // Ensure non-negative values for business metrics
        label: `Day ${index + 1}`,
      };
    });

    // Calculate confidence intervals (simplified approach)
    const validationMetrics = this.calculateValidationMetricsFromHistory();
    const predictionStd = Math.sqrt(validationMetrics.mse);
    
    const confidenceIntervals = {
      lower: denormalizedPredictions.map((pred, i) => {
        const uncertainty = predictionStd * Math.sqrt(1 + i * 0.1); // Increasing uncertainty
        return Math.max(0, pred - 1.96 * uncertainty);
      }),
      upper: denormalizedPredictions.map((pred, i) => {
        const uncertainty = predictionStd * Math.sqrt(1 + i * 0.1);
        return pred + 1.96 * uncertainty;
      }),
    };

    // Calculate model confidence score
    const modelConfidence = Math.min(1.0, Math.max(0.0, validationMetrics.r2));

    console.log(`‚úÖ Forecast generated: ${predictionPoints.length} predictions`);
    console.log(`üìä Confidence score: ${(modelConfidence * 100).toFixed(1)}%`);

    return {
      modelId: `lstm_${Date.now()}`,
      predictions: predictionPoints,
      confidenceIntervals,
      forecastOrigin: new Date(),
      horizon,
      metrics: validationMetrics,
      modelConfidence,
    };
  }

  // Calculate validation metrics
  private calculateValidationMetrics(actual: number[], predicted: number[]): ValidationMetrics {
    const n = actual.length;
    
    if (n === 0 || actual.length !== predicted.length) {
      throw new Error('Actual and predicted arrays must have the same non-zero length');
    }

    // Mean Absolute Error
    const mae = actual.reduce((sum, act, i) => sum + Math.abs(act - predicted[i]), 0) / n;

    // Mean Squared Error
    const mse = actual.reduce((sum, act, i) => sum + Math.pow(act - predicted[i], 2), 0) / n;

    // Root Mean Squared Error
    const rmse = Math.sqrt(mse);

    // Mean Absolute Percentage Error
    let mapeSum = 0;
    let mapeCount = 0;
    for (let i = 0; i < n; i++) {
      if (Math.abs(actual[i]) > 0.001) {
        mapeSum += Math.abs((actual[i] - predicted[i]) / actual[i]) * 100;
        mapeCount++;
      }
    }
    const mape = mapeCount > 0 ? mapeSum / mapeCount : 0;

    // R-squared
    const actualMean = actual.reduce((sum, val) => sum + val, 0) / n;
    const totalSumSquares = actual.reduce((sum, val) => sum + Math.pow(val - actualMean, 2), 0);
    const residualSumSquares = actual.reduce((sum, val, i) => sum + Math.pow(val - predicted[i], 2), 0);
    const r2 = totalSumSquares > 0 ? 1 - (residualSumSquares / totalSumSquares) : 0;

    // Directional Accuracy
    let correctDirections = 0;
    for (let i = 1; i < n; i++) {
      const actualDirection = actual[i] > actual[i - 1];
      const predictedDirection = predicted[i] > predicted[i - 1];
      if (actualDirection === predictedDirection) {
        correctDirections++;
      }
    }
    const directionalAccuracy = n > 1 ? (correctDirections / (n - 1)) * 100 : 0;

    // Residual statistics
    const residuals = actual.map((act, i) => act - predicted[i]);
    const residualMean = residuals.reduce((sum, res) => sum + res, 0) / n;
    const residualVariance = residuals.reduce((sum, res) => sum + Math.pow(res - residualMean, 2), 0) / n;
    const residualStd = Math.sqrt(residualVariance);

    // Skewness and Kurtosis (simplified)
    const skewness = residuals.reduce((sum, res) => sum + Math.pow((res - residualMean) / residualStd, 3), 0) / n;
    const kurtosis = residuals.reduce((sum, res) => sum + Math.pow((res - residualMean) / residualStd, 4), 0) / n - 3;

    return {
      mae,
      rmse,
      mape: Math.min(mape, 999), // Cap extreme values
      r2: Math.max(-5, Math.min(1, r2)), // Reasonable bounds
      mse,
      directionalAccuracy,
      residualStats: {
        mean: residualMean,
        std: residualStd,
        skewness,
        kurtosis,
      },
    };
  }

  // Get validation metrics from training history
  private calculateValidationMetricsFromHistory(): ValidationMetrics {
    // This is a simplified version - in practice, you'd use actual validation predictions
    const dummyMetrics: ValidationMetrics = {
      mae: 100,
      rmse: 150,
      mape: 15,
      r2: 0.75,
      mse: 22500,
      directionalAccuracy: 65,
      residualStats: {
        mean: 0,
        std: 150,
        skewness: 0.1,
        kurtosis: 0.2,
      },
    };
    
    return dummyMetrics;
  }

  // Get model summary
  private getModelSummary(): { totalParams: number; trainableParams: number } {
    if (!this.model) {
      return { totalParams: 0, trainableParams: 0 };
    }

    let totalParams = 0;
    let trainableParams = 0;

    for (const layer of this.model.layers) {
      const layerParams = layer.countParams();
      totalParams += layerParams;
      if (layer.trainable) {
        trainableParams += layerParams;
      }
    }

    return { totalParams, trainableParams };
  }

  // Dispose model and free memory
  dispose(): void {
    if (this.model) {
      this.model.dispose();
      this.model = null;
    }
    console.log('üóëÔ∏è LSTM model disposed');
  }

  // Auto model selection - try different configurations
  static async autoSelect(
    timeSeries: TimeSeriesDataPoint[],
    baseConfig: LSTMModelConfig,
    progressCallback?: (progress: { stage: string; progress: number; model: string }) => void
  ): Promise<{ bestModel: TrainedLSTMModel; bestConfig: LSTMModelConfig; results: TrainedLSTMModel[] }> {
    
    console.log('üîç LSTM Auto Model Selection - Testing multiple configurations...');
    
    const configurations: { params: LSTMParams; name: string }[] = [
      // Simple models
      {
        params: { lookBackDays: 7, epochs: 50, batchSize: 32, learningRate: 0.001, hiddenUnits: 32, dropout: 0.2, layers: 1, validationSplit: 0.2 },
        name: 'Simple LSTM (7d, 32u, 1L)'
      },
      {
        params: { lookBackDays: 14, epochs: 50, batchSize: 32, learningRate: 0.001, hiddenUnits: 64, dropout: 0.2, layers: 1, validationSplit: 0.2 },
        name: 'Medium LSTM (14d, 64u, 1L)'
      },
      // Complex models
      {
        params: { lookBackDays: 21, epochs: 100, batchSize: 16, learningRate: 0.0005, hiddenUnits: 128, dropout: 0.3, layers: 2, validationSplit: 0.2 },
        name: 'Deep LSTM (21d, 128u, 2L)'
      },
      {
        params: { lookBackDays: 30, epochs: 100, batchSize: 16, learningRate: 0.0003, hiddenUnits: 64, dropout: 0.25, layers: 2, validationSplit: 0.2 },
        name: 'Wide LSTM (30d, 64u, 2L)'
      },
      // Optimized models
      {
        params: { lookBackDays: 14, epochs: 75, batchSize: 24, learningRate: 0.0008, hiddenUnits: 96, dropout: 0.3, layers: 2, validationSplit: 0.2 },
        name: 'Optimized LSTM (14d, 96u, 2L)'
      },
    ];

    const results: TrainedLSTMModel[] = [];
    let bestModel: TrainedLSTMModel | null = null;
    let bestScore = -Infinity;
    let bestConfig: LSTMModelConfig = baseConfig;

    for (let i = 0; i < configurations.length; i++) {
      const { params, name } = configurations[i];
      const config = { ...baseConfig };
      
      try {
        if (progressCallback) {
          progressCallback({
            stage: 'training',
            progress: (i / configurations.length) * 100,
            model: name
          });
        }

        console.log(`üîß Training ${name}...`);

                  const model = new LSTMModel(params, config);
          const trainedModel = await model.train(timeSeries, (epoch, _logs) => {
          // Progress within this model
          const modelProgress = (epoch + 1) / params.epochs;
          const overallProgress = ((i + modelProgress) / configurations.length) * 100;
          
          if (progressCallback) {
            progressCallback({
              stage: 'training',
              progress: overallProgress,
              model: `${name} - Epoch ${epoch + 1}/${params.epochs}`
            });
          }
        });

        results.push(trainedModel);

        // Scoring system for model selection
        const metrics = trainedModel.validationMetrics;
        let score = 0;

        // Primary: R¬≤ score (weighted heavily)
        score += metrics.r2 * 100;

        // Secondary: MAPE penalty
        if (metrics.mape < 10) score += 20;
        else if (metrics.mape < 15) score += 10;
        else if (metrics.mape > 25) score -= 15;

        // Directional accuracy bonus
        if (metrics.directionalAccuracy > 60) score += 10;
        else if (metrics.directionalAccuracy > 70) score += 20;

        // Convergence bonus
        if (trainedModel.convergenceStatus === 'converged' || trainedModel.convergenceStatus === 'completed') {
          score += 5;
        }

        // Training time penalty (prefer faster models if similar performance)
        const trainingTimePenalty = Math.min(10, trainedModel.trainingTime / 60000); // Minutes
        score -= trainingTimePenalty;

        console.log(`üìä ${name}: R¬≤=${metrics.r2.toFixed(3)}, MAPE=${metrics.mape.toFixed(1)}%, DA=${metrics.directionalAccuracy.toFixed(1)}%, Score=${score.toFixed(1)}`);

        if (score > bestScore) {
          bestScore = score;
          bestModel = trainedModel;
          bestConfig = config;
        }

        // Cleanup
        model.dispose();

      } catch (error) {
        console.warn(`‚ùå ${name} failed:`, error);
      }
    }

    if (!bestModel) {
      throw new Error('All LSTM configurations failed. Data may be unsuitable for LSTM modeling.');
    }

    console.log(`üèÜ Best LSTM model: ${bestModel.name}`);
    console.log(`üìà Performance: R¬≤=${bestModel.validationMetrics.r2.toFixed(3)}, MAPE=${bestModel.validationMetrics.mape.toFixed(1)}%`);

    return {
      bestModel,
      bestConfig,
      results,
    };
  }
}
